{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring-2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Python version: 3.10.6\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set features (X) and labels (y)\n",
    "df_X = pd.read_csv(\"data/train_values.csv\", on_bad_lines=\"skip\", sep=\",\", header=0)\n",
    "df_y = pd.read_csv(\"data/train_labels.csv\", on_bad_lines=\"skip\", sep=\",\", header=0)\n",
    "\n",
    "# Convert labels to 0, 1, 2 for xgboost classifier\n",
    "df_y2 = df_y.copy()\n",
    "df_y2.loc[df_y2.damage_grade == 1, \"damage_grade\"] = 0\n",
    "df_y2.loc[df_y2.damage_grade == 2, \"damage_grade\"] = 1\n",
    "df_y2.loc[df_y2.damage_grade == 3, \"damage_grade\"] = 2\n",
    "\n",
    "# dataframe for test set features\n",
    "df_X_test = pd.read_csv(\"data/test_values.csv\", on_bad_lines=\"skip\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802906</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28830</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94947</td>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590882</td>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201944</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260596</th>\n",
       "      <td>688636</td>\n",
       "      <td>25</td>\n",
       "      <td>1335</td>\n",
       "      <td>1621</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260597</th>\n",
       "      <td>669485</td>\n",
       "      <td>17</td>\n",
       "      <td>715</td>\n",
       "      <td>2060</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260598</th>\n",
       "      <td>602512</td>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>8163</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260599</th>\n",
       "      <td>151409</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>1851</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260600</th>\n",
       "      <td>747594</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>9101</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>n</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260601 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id   \n",
       "0            802906               6             487           12198  \\\n",
       "1             28830               8             900            2812   \n",
       "2             94947              21             363            8973   \n",
       "3            590882              22             418           10694   \n",
       "4            201944              11             131            1488   \n",
       "...             ...             ...             ...             ...   \n",
       "260596       688636              25            1335            1621   \n",
       "260597       669485              17             715            2060   \n",
       "260598       602512              17              51            8163   \n",
       "260599       151409              26              39            1851   \n",
       "260600       747594              21               9            9101   \n",
       "\n",
       "        count_floors_pre_eq  age  area_percentage  height_percentage   \n",
       "0                         2   30                6                  5  \\\n",
       "1                         2   10                8                  7   \n",
       "2                         2   10                5                  5   \n",
       "3                         2   10                6                  5   \n",
       "4                         3   30                8                  9   \n",
       "...                     ...  ...              ...                ...   \n",
       "260596                    1   55                6                  3   \n",
       "260597                    2    0                6                  5   \n",
       "260598                    3   55                6                  7   \n",
       "260599                    2   10               14                  6   \n",
       "260600                    3   10                7                  6   \n",
       "\n",
       "       land_surface_condition foundation_type  ...   \n",
       "0                           t               r  ...  \\\n",
       "1                           o               r  ...   \n",
       "2                           t               r  ...   \n",
       "3                           t               r  ...   \n",
       "4                           t               r  ...   \n",
       "...                       ...             ...  ...   \n",
       "260596                      n               r  ...   \n",
       "260597                      t               r  ...   \n",
       "260598                      t               r  ...   \n",
       "260599                      t               r  ...   \n",
       "260600                      n               r  ...   \n",
       "\n",
       "       has_secondary_use_agriculture has_secondary_use_hotel   \n",
       "0                                  0                       0  \\\n",
       "1                                  0                       0   \n",
       "2                                  0                       0   \n",
       "3                                  0                       0   \n",
       "4                                  0                       0   \n",
       "...                              ...                     ...   \n",
       "260596                             0                       0   \n",
       "260597                             0                       0   \n",
       "260598                             0                       0   \n",
       "260599                             0                       0   \n",
       "260600                             0                       0   \n",
       "\n",
       "       has_secondary_use_rental has_secondary_use_institution   \n",
       "0                             0                             0  \\\n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "...                         ...                           ...   \n",
       "260596                        0                             0   \n",
       "260597                        0                             0   \n",
       "260598                        0                             0   \n",
       "260599                        0                             0   \n",
       "260600                        0                             0   \n",
       "\n",
       "       has_secondary_use_school  has_secondary_use_industry   \n",
       "0                             0                           0  \\\n",
       "1                             0                           0   \n",
       "2                             0                           0   \n",
       "3                             0                           0   \n",
       "4                             0                           0   \n",
       "...                         ...                         ...   \n",
       "260596                        0                           0   \n",
       "260597                        0                           0   \n",
       "260598                        0                           0   \n",
       "260599                        0                           0   \n",
       "260600                        0                           0   \n",
       "\n",
       "        has_secondary_use_health_post  has_secondary_use_gov_office   \n",
       "0                                   0                             0  \\\n",
       "1                                   0                             0   \n",
       "2                                   0                             0   \n",
       "3                                   0                             0   \n",
       "4                                   0                             0   \n",
       "...                               ...                           ...   \n",
       "260596                              0                             0   \n",
       "260597                              0                             0   \n",
       "260598                              0                             0   \n",
       "260599                              0                             0   \n",
       "260600                              0                             0   \n",
       "\n",
       "        has_secondary_use_use_police  has_secondary_use_other  \n",
       "0                                  0                        0  \n",
       "1                                  0                        0  \n",
       "2                                  0                        0  \n",
       "3                                  0                        0  \n",
       "4                                  0                        0  \n",
       "...                              ...                      ...  \n",
       "260596                             0                        0  \n",
       "260597                             0                        0  \n",
       "260598                             0                        0  \n",
       "260599                             0                        0  \n",
       "260600                             0                        0  \n",
       "\n",
       "[260601 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802906</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94947</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201944</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260596</th>\n",
       "      <td>688636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260597</th>\n",
       "      <td>669485</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260598</th>\n",
       "      <td>602512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260599</th>\n",
       "      <td>151409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260600</th>\n",
       "      <td>747594</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260601 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        building_id  damage_grade\n",
       "0            802906             2\n",
       "1             28830             1\n",
       "2             94947             2\n",
       "3            590882             1\n",
       "4            201944             2\n",
       "...             ...           ...\n",
       "260596       688636             1\n",
       "260597       669485             2\n",
       "260598       602512             2\n",
       "260599       151409             1\n",
       "260600       747594             2\n",
       "\n",
       "[260601 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Normalize numerical features\n",
    "Numerical features:\n",
    "- geo_level_1_id,\n",
    "- geo_level_2_id,\n",
    "- geo_level_3_id,\n",
    "- count_floors_pre_eq,\n",
    "- age,\n",
    "- area_percentage,\n",
    "- height_percentage,\n",
    "-  count_families\n",
    "\n",
    "Output: dataframes of normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Train set\n",
    "X_unscaled = df_X.loc[:, [\"geo_level_1_id\",\"geo_level_2_id\",\"geo_level_3_id\",\n",
    "                               \"count_floors_pre_eq\",\"age\",\"area_percentage\",\n",
    "                               \"height_percentage\", \"count_families\"]]\n",
    "\n",
    "X_normalized = pd.DataFrame(min_max_scaler.fit_transform(X_unscaled), columns=X_unscaled.columns)\n",
    "print(X_normalized)\n",
    "\n",
    "\n",
    "# Test set\n",
    "X_unscaled_test = df_X_test.loc[:, [\"geo_level_1_id\",\"geo_level_2_id\",\"geo_level_3_id\",\n",
    "                               \"count_floors_pre_eq\",\"age\",\"area_percentage\",\n",
    "                               \"height_percentage\", \"count_families\"]]\n",
    "\n",
    "X_normalized_test = pd.DataFrame(min_max_scaler.fit_transform(X_unscaled_test), columns=X_unscaled_test.columns)\n",
    "print(X_normalized_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Apply one-hot encoding to categorical features\n",
    "Categorical features:\n",
    "- land_surface_condition\n",
    "- foundation_type\n",
    "- roof_type\n",
    "- ground_floor_type\n",
    "- other_floor_type\n",
    "- position\n",
    "- plan_configuration\n",
    "- legal_ownership_status\n",
    "\n",
    "Output: dataframes of one-hot encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set:\n",
    "X_cat = df_X.loc[:, [\"land_surface_condition\", \"foundation_type\", \"roof_type\",\n",
    "                        \"ground_floor_type\", \"other_floor_type\", \"position\",\n",
    "                        \"plan_configuration\", \"legal_ownership_status\"]]     # 8 features\n",
    "X_cat = pd.get_dummies(X_cat,\n",
    "                          columns=[\"land_surface_condition\", \"foundation_type\", \"roof_type\",\n",
    "                                   \"ground_floor_type\", \"other_floor_type\", \"position\",\n",
    "                                   \"plan_configuration\", \"legal_ownership_status\"],\n",
    "                            sparse=True,\n",
    "                            dtype=int)  # Get SPARSE MATRIX VIA get_dummies()\n",
    "print(df_X_cat)\n",
    "\n",
    "\n",
    "\n",
    "# Test set:\n",
    "X_cat_test = X_test.loc[:, [\"land_surface_condition\", \"foundation_type\", \"roof_type\",\n",
    "                        \"ground_floor_type\", \"other_floor_type\", \"position\",\n",
    "                        \"plan_configuration\", \"legal_ownership_status\"]]     # 8 features\n",
    "X_cat_test = pd.get_dummies(X_cat_test,\n",
    "                          columns=[\"land_surface_condition\", \"foundation_type\", \"roof_type\",\n",
    "                                   \"ground_floor_type\", \"other_floor_type\", \"position\",\n",
    "                                   \"plan_configuration\", \"legal_ownership_status\"],\n",
    "                            sparse=True,\n",
    "                            dtype=int)  # Get SPARSE MATRIX VIA get_dummies()\n",
    "print(X_cat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Binary features\n",
    "Any feature that are not categorical nor numerical\n",
    "\n",
    "Output: dataframes of binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <th>has_superstructure_mud_mortar_brick</th>\n",
       "      <th>has_superstructure_cement_mortar_brick</th>\n",
       "      <th>has_superstructure_timber</th>\n",
       "      <th>has_superstructure_bamboo</th>\n",
       "      <th>has_superstructure_rc_non_engineered</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300051</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99355</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890251</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>421793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86863</th>\n",
       "      <td>310028</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86864</th>\n",
       "      <td>663567</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86865</th>\n",
       "      <td>1049160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86866</th>\n",
       "      <td>442785</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86867</th>\n",
       "      <td>501372</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86868 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       building_id  has_superstructure_adobe_mud   \n",
       "0           300051                             0  \\\n",
       "1            99355                             0   \n",
       "2           890251                             0   \n",
       "3           745817                             0   \n",
       "4           421793                             0   \n",
       "...            ...                           ...   \n",
       "86863       310028                             0   \n",
       "86864       663567                             1   \n",
       "86865      1049160                             0   \n",
       "86866       442785                             1   \n",
       "86867       501372                             0   \n",
       "\n",
       "       has_superstructure_mud_mortar_stone  has_superstructure_stone_flag   \n",
       "0                                        1                              0  \\\n",
       "1                                        1                              0   \n",
       "2                                        1                              0   \n",
       "3                                        0                              0   \n",
       "4                                        1                              0   \n",
       "...                                    ...                            ...   \n",
       "86863                                    1                              0   \n",
       "86864                                    1                              1   \n",
       "86865                                    1                              0   \n",
       "86866                                    1                              0   \n",
       "86867                                    0                              0   \n",
       "\n",
       "       has_superstructure_cement_mortar_stone   \n",
       "0                                           0  \\\n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "86863                                       0   \n",
       "86864                                       0   \n",
       "86865                                       0   \n",
       "86866                                       0   \n",
       "86867                                       0   \n",
       "\n",
       "       has_superstructure_mud_mortar_brick   \n",
       "0                                        0  \\\n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "...                                    ...   \n",
       "86863                                    0   \n",
       "86864                                    0   \n",
       "86865                                    0   \n",
       "86866                                    0   \n",
       "86867                                    0   \n",
       "\n",
       "       has_superstructure_cement_mortar_brick  has_superstructure_timber   \n",
       "0                                           0                          0  \\\n",
       "1                                           0                          0   \n",
       "2                                           0                          0   \n",
       "3                                           1                          0   \n",
       "4                                           0                          0   \n",
       "...                                       ...                        ...   \n",
       "86863                                       0                          1   \n",
       "86864                                       0                          0   \n",
       "86865                                       0                          1   \n",
       "86866                                       0                          0   \n",
       "86867                                       1                          0   \n",
       "\n",
       "       has_superstructure_bamboo  has_superstructure_rc_non_engineered  ...   \n",
       "0                              0                                     0  ...  \\\n",
       "1                              0                                     0  ...   \n",
       "2                              0                                     0  ...   \n",
       "3                              0                                     0  ...   \n",
       "4                              0                                     0  ...   \n",
       "...                          ...                                   ...  ...   \n",
       "86863                          0                                     0  ...   \n",
       "86864                          0                                     0  ...   \n",
       "86865                          0                                     0  ...   \n",
       "86866                          0                                     0  ...   \n",
       "86867                          0                                     0  ...   \n",
       "\n",
       "       has_secondary_use_agriculture  has_secondary_use_hotel   \n",
       "0                                  0                        0  \\\n",
       "1                                  1                        0   \n",
       "2                                  0                        0   \n",
       "3                                  0                        0   \n",
       "4                                  0                        0   \n",
       "...                              ...                      ...   \n",
       "86863                              1                        0   \n",
       "86864                              0                        0   \n",
       "86865                              0                        0   \n",
       "86866                              0                        0   \n",
       "86867                              0                        0   \n",
       "\n",
       "       has_secondary_use_rental  has_secondary_use_institution   \n",
       "0                             0                              0  \\\n",
       "1                             0                              0   \n",
       "2                             0                              0   \n",
       "3                             1                              0   \n",
       "4                             0                              0   \n",
       "...                         ...                            ...   \n",
       "86863                         0                              0   \n",
       "86864                         0                              0   \n",
       "86865                         0                              0   \n",
       "86866                         0                              0   \n",
       "86867                         0                              0   \n",
       "\n",
       "       has_secondary_use_school  has_secondary_use_industry   \n",
       "0                             0                           0  \\\n",
       "1                             0                           0   \n",
       "2                             0                           0   \n",
       "3                             0                           0   \n",
       "4                             0                           0   \n",
       "...                         ...                         ...   \n",
       "86863                         0                           0   \n",
       "86864                         0                           0   \n",
       "86865                         0                           0   \n",
       "86866                         0                           0   \n",
       "86867                         0                           0   \n",
       "\n",
       "       has_secondary_use_health_post  has_secondary_use_gov_office   \n",
       "0                                  0                             0  \\\n",
       "1                                  0                             0   \n",
       "2                                  0                             0   \n",
       "3                                  0                             0   \n",
       "4                                  0                             0   \n",
       "...                              ...                           ...   \n",
       "86863                              0                             0   \n",
       "86864                              0                             0   \n",
       "86865                              0                             0   \n",
       "86866                              0                             0   \n",
       "86867                              0                             0   \n",
       "\n",
       "       has_secondary_use_use_police  has_secondary_use_other  \n",
       "0                                 0                        0  \n",
       "1                                 0                        0  \n",
       "2                                 0                        0  \n",
       "3                                 0                        0  \n",
       "4                                 0                        0  \n",
       "...                             ...                      ...  \n",
       "86863                             0                        0  \n",
       "86864                             0                        0  \n",
       "86865                             0                        0  \n",
       "86866                             0                        0  \n",
       "86867                             0                        0  \n",
       "\n",
       "[86868 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set:\n",
    "X_bin = df_X.drop([\"building_id\", \"geo_level_1_id\",\"geo_level_2_id\",\"geo_level_3_id\",\n",
    "                        \"count_floors_pre_eq\",\"age\",\"area_percentage\",\n",
    "                        \"height_percentage\", \"count_families\",\"land_surface_condition\",\n",
    "                        \"foundation_type\",\n",
    "                        \"roof_type\",\n",
    "                        \"ground_floor_type\",\n",
    "                        \"other_floor_type\",\n",
    "                        \"position\",\n",
    "                        \"plan_configuration\",\n",
    "                        \"legal_ownership_status\"], axis=1)\n",
    "print(X_bin)\n",
    "\n",
    "\n",
    "# Test set:\n",
    "X_bin_test = df_X_test.drop([\"building_id\", \"geo_level_1_id\",\"geo_level_2_id\",\"geo_level_3_id\",\n",
    "                        \"count_floors_pre_eq\",\"age\",\"area_percentage\",\n",
    "                        \"height_percentage\", \"count_families\",\"land_surface_condition\",\n",
    "                        \"foundation_type\",\n",
    "                        \"roof_type\",\n",
    "                        \"ground_floor_type\",\n",
    "                        \"other_floor_type\",\n",
    "                        \"position\",\n",
    "                        \"plan_configuration\",\n",
    "                        \"legal_ownership_status\"], axis=1)\n",
    "print(X_bin_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Reconstruct dataframe of all features that are processed\n",
    "aka concatenating dataframes from 1.1, 1.2, and 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_normalized, X_bin, X_cat], axis=1)\n",
    "\n",
    "X_test = pd.concat([X_normalized_test, X_bin_test, X_cat_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Split to train - dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on these data to choose hyperparameters\n",
    "train_X = X_train[: int(len(X_train) * 0.85)]\n",
    "train_y = df_y[: int(len(df_y) * 0.85)].drop([\"building_id\"], axis=1)\n",
    "train_y2 = df_y2[: int(len(df_y2) * 0.85)].drop([\"building_id\"], axis=1)    # for xgboost\n",
    "\n",
    "dev_X = X_train[int(len(X_train) * 0.85 ):]\n",
    "dev_y = df_y[int(len(df_y) * 0.85 ):]\n",
    "dev_y2 = df_y2[int(len(df_y2) * 0.85 ):]    # for xgboost\n",
    "\n",
    "# Train model (with chosen hyperparameters) on this non-splitted data for inference on test set \n",
    "train_X_full = X_train\n",
    "train_y_full = df_y.drop([\"building_id\"], axis=1)\n",
    "train_y_full2 = df_y2.drop([\"building_id\"], axis=1) # df_y2 aka 0,1,2 labels so xgclassifier can work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Feature explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation among features\n",
    "\n",
    "cor = pd.concat([df_X_normalized, train_y], axis=1).corr()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cor, annot=True)\n",
    "\n",
    "# ---> Remove either height_percentage or count_floors_pre_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance threshold\n",
    "\n",
    "v_threshold = VarianceThreshold(threshold=0)\n",
    "v_threshold.fit(train_X)\n",
    "\n",
    "v_threshold.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information (Info gain)\n",
    "importances = mutual_info_classif(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "feat_importances = pd.Series(importances, train_X.columns)\n",
    "feat_importances.plot(kind=\"barh\", color=\"teal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 - Rebuild dataset\n",
    "Drop features that has low correlation to label: \n",
    "- count_floors_pre_eq\n",
    "- legal_ownership_status_w\n",
    "- legal_ownership_status_r\n",
    "- legal_ownership_status_a\n",
    "- plan_configuration_u\n",
    "- plan_configuration_s\n",
    "- plan_configuration_q\n",
    "- plan_configuration_o\n",
    "- plan_configuration_n\n",
    "- plan_configuration_m\n",
    "- plan_configuration_f\n",
    "- plan_configuration_c\n",
    "- plan_configuration_a\n",
    "- position_j\n",
    "- position_o\n",
    "- ground_floor_type_z\n",
    "- ground_floor_type_x\n",
    "- ground_floor_type_m\n",
    "- roof_type_q\n",
    "- foundation_type_h\n",
    "- land_surface_condition_o\n",
    "- land_surface_condition_n\n",
    "- has_secondary_use_other\n",
    "- has_secondary_use_use_police\n",
    "- has_secondary_use_gov_office\n",
    "- has_secondary_use_health_post\n",
    "- has_secondary_use_industry\n",
    "- has_secondary_use_school\n",
    "- has_secondary_use_institution\n",
    "- has_superstructure_other\n",
    "- has_superstructure_timber\n",
    "- has_superstructure_cement_mortar_stone\n",
    "- count_families\n",
    "    \n",
    "Drop features that has high correlation with one another: either height_percentage or count_floors_pre_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to be dropped:\n",
    "cols = [\n",
    "    \"count_floors_pre_eq\", \"legal_ownership_status_w\", \"legal_ownership_status_r\",\n",
    "    \"legal_ownership_status_a\", \"plan_configuration_u\", \"plan_configuration_s\",\n",
    "    \"plan_configuration_q\", \"plan_configuration_o\", \"plan_configuration_n\",\n",
    "    \"plan_configuration_m\", \"plan_configuration_f\", \"plan_configuration_c\",\n",
    "    \"plan_configuration_a\", \"position_j\", \"position_o\", \"ground_floor_type_z\",\n",
    "    \"ground_floor_type_x\", \"ground_floor_type_m\", \"roof_type_q\", \"foundation_type_h\",\n",
    "    \"land_surface_condition_o\", \"land_surface_condition_n\", \"has_secondary_use_other\",\n",
    "    \"has_secondary_use_use_police\", \"has_secondary_use_gov_office\", \"has_secondary_use_health_post\",\n",
    "    \"has_secondary_use_industry\", \"has_secondary_use_school\", \"has_secondary_use_institution\",\n",
    "    \"has_superstructure_other\", \"has_superstructure_timber\", \"has_superstructure_cement_mortar_stone\",\n",
    "    \"count_families\"\n",
    "]\n",
    "\n",
    "\n",
    "# Drop features on train-dev splits\n",
    "train_X_tuned = train_X.drop(cols, axis=1)\n",
    "dev_X_tuned = dev_X.drop(cols, axis=1)\n",
    "\n",
    "# Drop features on full train data\n",
    "train_X_full_tuned = train_X_full.drop(cols, axis=1)\n",
    "\n",
    "# Drop features on test data\n",
    "X_test_dropped = X_test.drop(cols, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model try out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write prediction to pred.csv\n",
    "def write_out(test_set, result, xgboost=False):\n",
    "    final_result = []\n",
    "    for i,j in zip(test_set.loc[:,[\"building_id\"]].values, result):\n",
    "        if xgboost:\n",
    "            final_result.append([i[0],j+1])\n",
    "        else:\n",
    "            final_result.append([i[0],j])\n",
    "        \n",
    "    output = pd.DataFrame(final_result, columns=[\"building_id\",\"damage_grade\"])\n",
    "    # final_result\n",
    "    output.to_csv(\"pred.csv\", index=False)\n",
    "\n",
    "# Calculate F1 scores\n",
    "def val_acc(dev_set, result):\n",
    "    score = f1_score(dev_set[\"damage_grade\"].values.tolist(),\n",
    "                     result, average='micro')\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Random forest classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a - Train on dataset of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == \"\":\n",
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.844699731389102"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=101, min_samples_split=15,\n",
    "                             min_samples_leaf=1, max_depth=50,\n",
    "                             criterion=\"entropy\", bootstrap=True)\n",
    "clf.fit(train_X, train_y)\n",
    "clf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7200360699142381"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clf.predict(dev_X).tolist()\n",
    "val_acc(dev_y, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b - Train on dataset of dropped features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == \"\":\n",
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8389054105909439"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=101, min_samples_split=15,\n",
    "                             min_samples_leaf=1, max_depth=50,\n",
    "                             criterion=\"entropy\", bootstrap=True)\n",
    "clf2.fit(train_X_tuned, train_y)\n",
    "clf2.score(train_X_tuned, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7206692120258629"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clf2.predict(dev_X_tuned).tolist()\n",
    "val_acc(dev_y, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    }
   ],
   "source": [
    "result = clf2.predict(X_test_dropped).tolist()\n",
    "write_out(df_X_test, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(n_estimators=101, min_samples_split=15,\n",
    "#                     min_samples_leaf=1, max_depth=50,\n",
    "#                     criterion=\"entropy\", bootstrap=True)\n",
    "# adab_clf = AdaBoostClassifier(n_estimators=100, base_estimator=rf_clf)\n",
    "# adab_clf.fit(train_X_tuned, train_y)\n",
    "\n",
    "# adab_clf.score(train_X_tuned, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieuc\\Documents\\Projects\\USC\\Spring 2023\\CSCI567\\Earthquake-competition\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:625: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6567986032501295"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = adab_clf.predict(dev_X_tuned)\n",
    "# val_acc(dev_y, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - XGBoost Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1a - Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on dataset of all features\n",
    "xgb_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "xgb_clf.fit(train_X, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7274994723815736"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = xgb_clf.predict(dev_X)\n",
    "val_acc(dev_y2, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on filtered features dataset\n",
    "xgb_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "xgb_clf.fit(train_X_tuned, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729418084841043"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = xgb_clf.predict(dev_X_tuned)\n",
    "val_acc(dev_y2, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1b - Train on full dataset for inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on filtered features dataset\n",
    "xgb_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "\n",
    "xgb_clf.fit(train_X_full_tuned, train_y_full2)\n",
    "print(xgb_clf.score(train_X_full_tuned, train_y_full2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7416663788703803"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = xgb_clf.predict(X_test_dropped).tolist()\n",
    "write_out(df_X_test, result, xgboost=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Finetune model with Searching techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 - Optimal hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-06 19:20:13,899]\u001b[0m A new study created in memory with name: no-name-ea202a06-155d-4653-888e-9fef1a994f03\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:24,705]\u001b[0m Trial 0 finished with value: 0.7253843595712568 and parameters: {'max_depth': 7, 'learning_rate': 0.9438354107228637, 'n_estimators': 444, 'min_child_weight': 9, 'gamma': 0.6223055300165437, 'subsample': 0.6250200569455229, 'colsample_bytree': 0.9975220477388381, 'reg_alpha': 0.3217872306883284, 'reg_lambda': 0.24182896340611398}. Best is trial 0 with value: 0.7253843595712568.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:28,521]\u001b[0m Trial 1 finished with value: 0.7340308510910438 and parameters: {'max_depth': 7, 'learning_rate': 0.6939403033724492, 'n_estimators': 316, 'min_child_weight': 4, 'gamma': 0.9319627806570424, 'subsample': 0.9166304591359962, 'colsample_bytree': 0.7639471405654891, 'reg_alpha': 0.0591778394310043, 'reg_lambda': 0.07481073751318161}. Best is trial 1 with value: 0.7340308510910438.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:34,138]\u001b[0m Trial 2 finished with value: 0.7389168862397995 and parameters: {'max_depth': 7, 'learning_rate': 0.521587321592489, 'n_estimators': 280, 'min_child_weight': 8, 'gamma': 0.9727335571688307, 'subsample': 0.6196552722742844, 'colsample_bytree': 0.8035730827326244, 'reg_alpha': 0.5279827473155675, 'reg_lambda': 0.03243350740215251}. Best is trial 2 with value: 0.7389168862397995.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:41,325]\u001b[0m Trial 3 finished with value: 0.7407331610856719 and parameters: {'max_depth': 7, 'learning_rate': 0.5328883394992335, 'n_estimators': 350, 'min_child_weight': 10, 'gamma': 0.6716355230308464, 'subsample': 0.6763651622304256, 'colsample_bytree': 0.5588405907255638, 'reg_alpha': 0.8611965634438709, 'reg_lambda': 0.41618906435202857}. Best is trial 3 with value: 0.7407331610856719.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:51,277]\u001b[0m Trial 4 finished with value: 0.7353866618914839 and parameters: {'max_depth': 6, 'learning_rate': 0.4192410103740686, 'n_estimators': 482, 'min_child_weight': 7, 'gamma': 0.28785996309707773, 'subsample': 0.4729611421200454, 'colsample_bytree': 0.9797694985843428, 'reg_alpha': 0.6970817915206269, 'reg_lambda': 0.7755815977574634}. Best is trial 3 with value: 0.7407331610856719.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:51,903]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:52,457]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:20:57,559]\u001b[0m Trial 7 finished with value: 0.7396075823079481 and parameters: {'max_depth': 12, 'learning_rate': 0.4938037536715608, 'n_estimators': 296, 'min_child_weight': 5, 'gamma': 0.9548945776704126, 'subsample': 0.79246337800187, 'colsample_bytree': 0.5266587223385217, 'reg_alpha': 0.1466581644978469, 'reg_lambda': 0.5752703517105157}. Best is trial 3 with value: 0.7407331610856719.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:02,949]\u001b[0m Trial 8 finished with value: 0.7371261927297844 and parameters: {'max_depth': 14, 'learning_rate': 0.6710264245833029, 'n_estimators': 195, 'min_child_weight': 9, 'gamma': 0.9030610763202152, 'subsample': 0.7171906357382865, 'colsample_bytree': 0.9305567941875014, 'reg_alpha': 0.6003836928109821, 'reg_lambda': 0.6817720672247329}. Best is trial 3 with value: 0.7407331610856719.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:10,094]\u001b[0m Trial 9 finished with value: 0.7424726919239723 and parameters: {'max_depth': 9, 'learning_rate': 0.44142589165970403, 'n_estimators': 279, 'min_child_weight': 10, 'gamma': 0.43554658374915073, 'subsample': 0.8416544875385414, 'colsample_bytree': 0.7986955763269341, 'reg_alpha': 0.44172259063921526, 'reg_lambda': 0.06428780407675158}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:21,838]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 72.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:22,258]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:22,687]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:25,504]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 100.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:34,799]\u001b[0m Trial 14 finished with value: 0.7392238622700879 and parameters: {'max_depth': 8, 'learning_rate': 0.6219974980619855, 'n_estimators': 474, 'min_child_weight': 8, 'gamma': 0.13218353235887564, 'subsample': 0.9671973472446145, 'colsample_bytree': 0.9095161662145179, 'reg_alpha': 0.911551864453249, 'reg_lambda': 0.1489993814809646}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:38,799]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 119.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:46,048]\u001b[0m Trial 16 finished with value: 0.7400168836816658 and parameters: {'max_depth': 13, 'learning_rate': 0.430231963685041, 'n_estimators': 493, 'min_child_weight': 1, 'gamma': 0.8610285148064788, 'subsample': 0.8802190652224764, 'colsample_bytree': 0.9477054108765712, 'reg_alpha': 0.662991180325868, 'reg_lambda': 0.33552606274454505}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:46,490]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:48,281]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 58.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:54,399]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 98.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:54,973]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:21:55,516]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:01,064]\u001b[0m Trial 22 finished with value: 0.7416285078406795 and parameters: {'max_depth': 10, 'learning_rate': 0.5457466209649344, 'n_estimators': 349, 'min_child_weight': 10, 'gamma': 0.7420982089744285, 'subsample': 0.8585909973153352, 'colsample_bytree': 0.8018034151114938, 'reg_alpha': 0.7605392350808505, 'reg_lambda': 0.438778603828786}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:06,708]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 286.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:07,289]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:07,983]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:08,488]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:17,384]\u001b[0m Trial 27 finished with value: 0.7385075848660817 and parameters: {'max_depth': 11, 'learning_rate': 0.4898040989947539, 'n_estimators': 260, 'min_child_weight': 7, 'gamma': 0.5188377032957987, 'subsample': 0.7401385549148096, 'colsample_bytree': 0.7182879105955907, 'reg_alpha': 0.9344265402326675, 'reg_lambda': 0.437644821231827}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:17,886]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:18,368]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:20,397]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 47.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:28,004]\u001b[0m Trial 31 finished with value: 0.7405540917346704 and parameters: {'max_depth': 12, 'learning_rate': 0.4202088936751673, 'n_estimators': 459, 'min_child_weight': 9, 'gamma': 0.81814097771039, 'subsample': 0.8516049938064769, 'colsample_bytree': 0.8878673139791248, 'reg_alpha': 0.7830321640323086, 'reg_lambda': 0.3424342093946938}. Best is trial 9 with value: 0.7424726919239723.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:30,898]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 135.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:36,276]\u001b[0m Trial 33 finished with value: 0.7427540866184033 and parameters: {'max_depth': 12, 'learning_rate': 0.39183533883581934, 'n_estimators': 323, 'min_child_weight': 9, 'gamma': 0.8199904894315466, 'subsample': 0.8749540180090918, 'colsample_bytree': 0.7802482398988517, 'reg_alpha': 0.8714734596460376, 'reg_lambda': 0.21703360462435475}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:41,475]\u001b[0m Trial 34 finished with value: 0.7427285052825457 and parameters: {'max_depth': 12, 'learning_rate': 0.38445176622350824, 'n_estimators': 324, 'min_child_weight': 9, 'gamma': 0.8074633981667637, 'subsample': 0.8825847912208595, 'colsample_bytree': 0.7672182129693859, 'reg_alpha': 0.9107915500322727, 'reg_lambda': 0.2221057017934135}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:44,110]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 65.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:44,912]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:51,754]\u001b[0m Trial 37 finished with value: 0.74152618249725 and parameters: {'max_depth': 14, 'learning_rate': 0.32065869465187585, 'n_estimators': 281, 'min_child_weight': 9, 'gamma': 0.7183726926113307, 'subsample': 0.8042733038292099, 'colsample_bytree': 0.7514758949059185, 'reg_alpha': 0.7312419757575792, 'reg_lambda': 0.05711215186558238}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:52,461]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:22:57,014]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 273.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:03,801]\u001b[0m Trial 40 finished with value: 0.7402215343685247 and parameters: {'max_depth': 14, 'learning_rate': 0.4509422746422251, 'n_estimators': 279, 'min_child_weight': 8, 'gamma': 0.6507272586550491, 'subsample': 0.8076343383813838, 'colsample_bytree': 0.8198629264094335, 'reg_alpha': 0.6702118173511961, 'reg_lambda': 0.011639359146582033}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:10,380]\u001b[0m Trial 41 finished with value: 0.7416540891765367 and parameters: {'max_depth': 15, 'learning_rate': 0.33241675356542966, 'n_estimators': 278, 'min_child_weight': 9, 'gamma': 0.7157802927482892, 'subsample': 0.8205214087208144, 'colsample_bytree': 0.7486144250656728, 'reg_alpha': 0.7398304914194185, 'reg_lambda': 0.0766266093956694}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:15,572]\u001b[0m Trial 42 finished with value: 0.7421912972295415 and parameters: {'max_depth': 15, 'learning_rate': 0.32109448091369813, 'n_estimators': 230, 'min_child_weight': 9, 'gamma': 0.9274418854762652, 'subsample': 0.8403878137096772, 'colsample_bytree': 0.777427973685826, 'reg_alpha': 0.8282442207358656, 'reg_lambda': 0.06495325178628836}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:20,801]\u001b[0m Trial 43 finished with value: 0.7413982758179632 and parameters: {'max_depth': 15, 'learning_rate': 0.3259114857015074, 'n_estimators': 211, 'min_child_weight': 9, 'gamma': 0.9353428112467718, 'subsample': 0.7712860882722448, 'colsample_bytree': 0.7452730832558296, 'reg_alpha': 0.8274341618475048, 'reg_lambda': 0.04727217461066058}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:25,920]\u001b[0m Trial 44 finished with value: 0.7417564145199663 and parameters: {'max_depth': 15, 'learning_rate': 0.2603405471159952, 'n_estimators': 183, 'min_child_weight': 8, 'gamma': 0.9169049927162415, 'subsample': 0.8256041951976317, 'colsample_bytree': 0.771741923048249, 'reg_alpha': 0.9283038972201644, 'reg_lambda': 0.10808656724125734}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:30,323]\u001b[0m Trial 45 finished with value: 0.7421145532219692 and parameters: {'max_depth': 14, 'learning_rate': 0.2600566186258633, 'n_estimators': 160, 'min_child_weight': 8, 'gamma': 0.9395057353760223, 'subsample': 0.8974145804441361, 'colsample_bytree': 0.768898494581328, 'reg_alpha': 0.898329328689517, 'reg_lambda': 0.12110122791222913}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:33,588]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 76.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:37,736]\u001b[0m Trial 47 finished with value: 0.7413726944821059 and parameters: {'max_depth': 13, 'learning_rate': 0.2823591849784686, 'n_estimators': 168, 'min_child_weight': 9, 'gamma': 0.855835311404219, 'subsample': 0.8981029139403469, 'colsample_bytree': 0.7017133796123924, 'reg_alpha': 0.8973773079616598, 'reg_lambda': 0.003391066199516804}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:40,405]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 51.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:45,867]\u001b[0m Trial 49 finished with value: 0.7402982783760969 and parameters: {'max_depth': 13, 'learning_rate': 0.2957301962063145, 'n_estimators': 301, 'min_child_weight': 8, 'gamma': 0.9352391326832261, 'subsample': 0.8848333255531912, 'colsample_bytree': 0.8586605153813459, 'reg_alpha': 0.8332375621092797, 'reg_lambda': 0.1823884551722743}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:46,477]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:51,247]\u001b[0m Trial 51 finished with value: 0.739530838300376 and parameters: {'max_depth': 15, 'learning_rate': 0.25342961257934465, 'n_estimators': 179, 'min_child_weight': 8, 'gamma': 0.9999967092343723, 'subsample': 0.8435369082834344, 'colsample_bytree': 0.7637514024889948, 'reg_alpha': 0.946901812410587, 'reg_lambda': 0.1200593398052898}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:56,576]\u001b[0m Trial 52 finished with value: 0.74152618249725 and parameters: {'max_depth': 15, 'learning_rate': 0.2799273765868204, 'n_estimators': 204, 'min_child_weight': 8, 'gamma': 0.9003706300664227, 'subsample': 0.8203224675432014, 'colsample_bytree': 0.7696910466690531, 'reg_alpha': 0.8878150874276112, 'reg_lambda': 0.08379401902135497}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:23:59,544]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 66.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:00,818]\u001b[0m Trial 54 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:05,785]\u001b[0m Trial 55 finished with value: 0.7416285078406795 and parameters: {'max_depth': 13, 'learning_rate': 0.29675715248774404, 'n_estimators': 225, 'min_child_weight': 5, 'gamma': 0.8285624137449055, 'subsample': 0.903775846157417, 'colsample_bytree': 0.8337177389160717, 'reg_alpha': 0.9636921310453264, 'reg_lambda': 0.18774998943427362}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:09,850]\u001b[0m Trial 56 finished with value: 0.7407331610856719 and parameters: {'max_depth': 15, 'learning_rate': 0.3986310959559251, 'n_estimators': 124, 'min_child_weight': 8, 'gamma': 0.8538175549722604, 'subsample': 0.7842223373457556, 'colsample_bytree': 0.7601917398356695, 'reg_alpha': 0.9110244210968466, 'reg_lambda': 0.2289623639639836}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:12,576]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 62.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:13,045]\u001b[0m Trial 58 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:17,740]\u001b[0m Trial 59 finished with value: 0.7419610652068251 and parameters: {'max_depth': 14, 'learning_rate': 0.27543994014401235, 'n_estimators': 204, 'min_child_weight': 7, 'gamma': 0.9569515970185721, 'subsample': 0.8823270361424084, 'colsample_bytree': 0.7350680112742578, 'reg_alpha': 0.9871851471786417, 'reg_lambda': 0.03460706950508233}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:18,748]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:19,733]\u001b[0m Trial 61 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:24,867]\u001b[0m Trial 62 finished with value: 0.7417819958558236 and parameters: {'max_depth': 15, 'learning_rate': 0.27607290608097945, 'n_estimators': 162, 'min_child_weight': 8, 'gamma': 0.7886586605091789, 'subsample': 0.8340624650545793, 'colsample_bytree': 0.7821874423470252, 'reg_alpha': 0.9983813734156619, 'reg_lambda': 0.0364748199122325}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:25,734]\u001b[0m Trial 63 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:30,152]\u001b[0m Trial 64 finished with value: 0.7409633931083881 and parameters: {'max_depth': 15, 'learning_rate': 0.3239497967428677, 'n_estimators': 162, 'min_child_weight': 9, 'gamma': 0.8560311664603473, 'subsample': 0.8701860626023039, 'colsample_bytree': 0.7863303835270165, 'reg_alpha': 0.8556730846185006, 'reg_lambda': 0.03441525517682517}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:30,691]\u001b[0m Trial 65 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:31,236]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:31,922]\u001b[0m Trial 67 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:32,419]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:32,942]\u001b[0m Trial 69 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:33,767]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:38,456]\u001b[0m Trial 71 finished with value: 0.7412703691386764 and parameters: {'max_depth': 15, 'learning_rate': 0.26753818254982215, 'n_estimators': 174, 'min_child_weight': 8, 'gamma': 0.9097068054792427, 'subsample': 0.8845292366149403, 'colsample_bytree': 0.773183909966556, 'reg_alpha': 0.9252130522095701, 'reg_lambda': 0.0970054130091911}. Best is trial 33 with value: 0.7427540866184033.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:43,529]\u001b[0m Trial 72 finished with value: 0.7427796679542605 and parameters: {'max_depth': 15, 'learning_rate': 0.26711764091827134, 'n_estimators': 189, 'min_child_weight': 8, 'gamma': 0.9306335587956946, 'subsample': 0.8273273505113233, 'colsample_bytree': 0.7959650212979125, 'reg_alpha': 0.9943348742698346, 'reg_lambda': 0.118472976972099}. Best is trial 72 with value: 0.7427796679542605.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:48,296]\u001b[0m Trial 73 finished with value: 0.7424982732598295 and parameters: {'max_depth': 15, 'learning_rate': 0.320737376616482, 'n_estimators': 198, 'min_child_weight': 9, 'gamma': 0.9517726723990212, 'subsample': 0.8113001905037797, 'colsample_bytree': 0.7917619087462502, 'reg_alpha': 0.9686311972089894, 'reg_lambda': 0.20986607398435336}. Best is trial 72 with value: 0.7427796679542605.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:49,213]\u001b[0m Trial 74 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:49,873]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:53,409]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 120.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:55,804]\u001b[0m Trial 77 pruned. Trial was pruned at iteration 36.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:56,352]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:56,882]\u001b[0m Trial 79 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:24:57,422]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:02,535]\u001b[0m Trial 81 finished with value: 0.7432912946714079 and parameters: {'max_depth': 15, 'learning_rate': 0.27760751635005676, 'n_estimators': 155, 'min_child_weight': 8, 'gamma': 0.8347152239063129, 'subsample': 0.8307414336037796, 'colsample_bytree': 0.8061030571151392, 'reg_alpha': 0.9982242270587781, 'reg_lambda': 0.025871348670803797}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:07,474]\u001b[0m Trial 82 finished with value: 0.7410657184518175 and parameters: {'max_depth': 15, 'learning_rate': 0.2876794760049364, 'n_estimators': 174, 'min_child_weight': 7, 'gamma': 0.8443257701879096, 'subsample': 0.8484470571188119, 'colsample_bytree': 0.8100501828699256, 'reg_alpha': 0.9438939658344783, 'reg_lambda': 0.09059780156320066}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:08,535]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:13,706]\u001b[0m Trial 84 finished with value: 0.7424215292522576 and parameters: {'max_depth': 15, 'learning_rate': 0.32806608577652496, 'n_estimators': 194, 'min_child_weight': 9, 'gamma': 0.8347093260117738, 'subsample': 0.8213174688682519, 'colsample_bytree': 0.778004201225358, 'reg_alpha': 0.8956442597419155, 'reg_lambda': 0.021964854283583704}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:20,458]\u001b[0m Trial 85 finished with value: 0.7407587424215293 and parameters: {'max_depth': 15, 'learning_rate': 0.33234794730200684, 'n_estimators': 343, 'min_child_weight': 9, 'gamma': 0.8121032430538973, 'subsample': 0.8220911731535513, 'colsample_bytree': 0.7784984659605538, 'reg_alpha': 0.8860568025634061, 'reg_lambda': 0.17709452851552476}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:27,130]\u001b[0m Trial 86 finished with value: 0.7429331559694047 and parameters: {'max_depth': 15, 'learning_rate': 0.3109783586118865, 'n_estimators': 235, 'min_child_weight': 10, 'gamma': 0.8350805451511947, 'subsample': 0.7581476838658383, 'colsample_bytree': 0.816201142265389, 'reg_alpha': 0.9146420343630886, 'reg_lambda': 0.01667072409948904}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:33,385]\u001b[0m Trial 87 finished with value: 0.7417564145199663 and parameters: {'max_depth': 15, 'learning_rate': 0.41013459184692314, 'n_estimators': 238, 'min_child_weight': 10, 'gamma': 0.7977343987508936, 'subsample': 0.7537152634689579, 'colsample_bytree': 0.8254776705943453, 'reg_alpha': 0.840478883245894, 'reg_lambda': 0.013364463686432116}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:39,257]\u001b[0m Trial 88 finished with value: 0.7423959479164002 and parameters: {'max_depth': 15, 'learning_rate': 0.37054716818391153, 'n_estimators': 261, 'min_child_weight': 10, 'gamma': 0.835274092490067, 'subsample': 0.8045627612435172, 'colsample_bytree': 0.8422185333041597, 'reg_alpha': 0.9116526524086025, 'reg_lambda': 0.058367294836527125}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:45,466]\u001b[0m Trial 89 finished with value: 0.7425494359315443 and parameters: {'max_depth': 15, 'learning_rate': 0.38917024835532765, 'n_estimators': 261, 'min_child_weight': 10, 'gamma': 0.752723117328371, 'subsample': 0.7945507089696111, 'colsample_bytree': 0.8430875190270157, 'reg_alpha': 0.9104089218095116, 'reg_lambda': 0.021890399046417666}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:52,259]\u001b[0m Trial 90 finished with value: 0.7424982732598295 and parameters: {'max_depth': 15, 'learning_rate': 0.39045984063248085, 'n_estimators': 288, 'min_child_weight': 10, 'gamma': 0.7570984885299048, 'subsample': 0.7829964345630658, 'colsample_bytree': 0.8563642764272509, 'reg_alpha': 0.9484423895848932, 'reg_lambda': 0.024762515314693118}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:25:56,679]\u001b[0m Trial 91 pruned. Trial was pruned at iteration 132.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:03,449]\u001b[0m Trial 92 finished with value: 0.7397099076513776 and parameters: {'max_depth': 15, 'learning_rate': 0.4361904562966328, 'n_estimators': 290, 'min_child_weight': 10, 'gamma': 0.7528365076089487, 'subsample': 0.7774411278961737, 'colsample_bytree': 0.8099091224209902, 'reg_alpha': 0.8693046852981504, 'reg_lambda': 0.0021821858146327377}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:06,347]\u001b[0m Trial 93 pruned. Trial was pruned at iteration 45.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:14,096]\u001b[0m Trial 94 finished with value: 0.7420889718861119 and parameters: {'max_depth': 15, 'learning_rate': 0.3618149944651377, 'n_estimators': 289, 'min_child_weight': 10, 'gamma': 0.694485478981989, 'subsample': 0.7625354667262954, 'colsample_bytree': 0.8314139938953106, 'reg_alpha': 0.9050725776206957, 'reg_lambda': 0.04896997990889845}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:14,653]\u001b[0m Trial 95 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:15,170]\u001b[0m Trial 96 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:15,898]\u001b[0m Trial 97 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:16,499]\u001b[0m Trial 98 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:16,998]\u001b[0m Trial 99 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:17,523]\u001b[0m Trial 100 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:19,339]\u001b[0m Trial 101 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:21,564]\u001b[0m Trial 102 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:22,352]\u001b[0m Trial 103 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:24,372]\u001b[0m Trial 104 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:24,935]\u001b[0m Trial 105 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:26,915]\u001b[0m Trial 106 pruned. Trial was pruned at iteration 18.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:27,561]\u001b[0m Trial 107 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:28,606]\u001b[0m Trial 108 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:29,235]\u001b[0m Trial 109 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:32,370]\u001b[0m Trial 110 pruned. Trial was pruned at iteration 33.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:34,222]\u001b[0m Trial 111 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:35,117]\u001b[0m Trial 112 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:35,762]\u001b[0m Trial 113 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:36,545]\u001b[0m Trial 114 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:37,171]\u001b[0m Trial 115 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:38,213]\u001b[0m Trial 116 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:38,782]\u001b[0m Trial 117 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:41,060]\u001b[0m Trial 118 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:41,646]\u001b[0m Trial 119 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:42,170]\u001b[0m Trial 120 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:43,045]\u001b[0m Trial 121 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:44,256]\u001b[0m Trial 122 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:45,086]\u001b[0m Trial 123 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:45,761]\u001b[0m Trial 124 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:46,921]\u001b[0m Trial 125 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:47,487]\u001b[0m Trial 126 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:48,161]\u001b[0m Trial 127 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:50,412]\u001b[0m Trial 128 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:53,302]\u001b[0m Trial 129 pruned. Trial was pruned at iteration 61.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:26:53,793]\u001b[0m Trial 130 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:02,013]\u001b[0m Trial 131 finished with value: 0.74152618249725 and parameters: {'max_depth': 15, 'learning_rate': 0.36075501261010556, 'n_estimators': 292, 'min_child_weight': 10, 'gamma': 0.6792938647319213, 'subsample': 0.7614485046176144, 'colsample_bytree': 0.8341699881878435, 'reg_alpha': 0.9058306006470181, 'reg_lambda': 0.04890319432868194}. Best is trial 81 with value: 0.7432912946714079.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:07,143]\u001b[0m Trial 132 finished with value: 0.7437261773809828 and parameters: {'max_depth': 15, 'learning_rate': 0.3764540392832544, 'n_estimators': 170, 'min_child_weight': 10, 'gamma': 0.7850461141396653, 'subsample': 0.769790325052997, 'colsample_bytree': 0.7996590788637075, 'reg_alpha': 0.8943210202920484, 'reg_lambda': 0.018781938241995533}. Best is trial 132 with value: 0.7437261773809828.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:09,520]\u001b[0m Trial 133 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:11,711]\u001b[0m Trial 134 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:12,605]\u001b[0m Trial 135 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:13,204]\u001b[0m Trial 136 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:19,206]\u001b[0m Trial 137 finished with value: 0.7432145506638357 and parameters: {'max_depth': 15, 'learning_rate': 0.28736271250901135, 'n_estimators': 171, 'min_child_weight': 9, 'gamma': 0.7153289286234618, 'subsample': 0.7887481374807213, 'colsample_bytree': 0.7895586793340191, 'reg_alpha': 0.9491584815730881, 'reg_lambda': 0.003557010142489235}. Best is trial 132 with value: 0.7437261773809828.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:20,115]\u001b[0m Trial 138 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:20,604]\u001b[0m Trial 139 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:21,147]\u001b[0m Trial 140 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:21,717]\u001b[0m Trial 141 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:22,861]\u001b[0m Trial 142 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:24,067]\u001b[0m Trial 143 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:24,679]\u001b[0m Trial 144 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:25,410]\u001b[0m Trial 145 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:30,997]\u001b[0m Trial 146 pruned. Trial was pruned at iteration 113.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:31,583]\u001b[0m Trial 147 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:34,468]\u001b[0m Trial 148 pruned. Trial was pruned at iteration 56.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:41,373]\u001b[0m Trial 149 finished with value: 0.7426261799391163 and parameters: {'max_depth': 15, 'learning_rate': 0.31849551246174046, 'n_estimators': 307, 'min_child_weight': 9, 'gamma': 0.7611635926600431, 'subsample': 0.8075227312740672, 'colsample_bytree': 0.7899945026763145, 'reg_alpha': 0.9365025410916676, 'reg_lambda': 0.11409993346798393}. Best is trial 132 with value: 0.7437261773809828.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:47,994]\u001b[0m Trial 150 finished with value: 0.7407843237573866 and parameters: {'max_depth': 15, 'learning_rate': 0.4033859211993651, 'n_estimators': 313, 'min_child_weight': 9, 'gamma': 0.7536925793074788, 'subsample': 0.8054357858489548, 'colsample_bytree': 0.7891596696064945, 'reg_alpha': 0.9328040137386916, 'reg_lambda': 0.1997481270838814}. Best is trial 132 with value: 0.7437261773809828.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:27:54,766]\u001b[0m Trial 151 finished with value: 0.7450819881814228 and parameters: {'max_depth': 15, 'learning_rate': 0.31079718234242176, 'n_estimators': 302, 'min_child_weight': 9, 'gamma': 0.7690739730134937, 'subsample': 0.8146642478947701, 'colsample_bytree': 0.8004949225291711, 'reg_alpha': 0.8961352915576605, 'reg_lambda': 0.11635687958658758}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:01,838]\u001b[0m Trial 152 finished with value: 0.7420378092143972 and parameters: {'max_depth': 15, 'learning_rate': 0.31202921704131986, 'n_estimators': 325, 'min_child_weight': 9, 'gamma': 0.7675568578820244, 'subsample': 0.8225552399301409, 'colsample_bytree': 0.8000278468083947, 'reg_alpha': 0.9812329501315622, 'reg_lambda': 0.11028527802765417}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:08,730]\u001b[0m Trial 153 finished with value: 0.7432145506638357 and parameters: {'max_depth': 15, 'learning_rate': 0.33885491222765185, 'n_estimators': 298, 'min_child_weight': 9, 'gamma': 0.7919012949320581, 'subsample': 0.7856093263193765, 'colsample_bytree': 0.8260617220147777, 'reg_alpha': 0.7157936595204777, 'reg_lambda': 0.04123219230518504}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:11,513]\u001b[0m Trial 154 pruned. Trial was pruned at iteration 32.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:12,108]\u001b[0m Trial 155 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:12,665]\u001b[0m Trial 156 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:19,295]\u001b[0m Trial 157 finished with value: 0.7423447852446855 and parameters: {'max_depth': 15, 'learning_rate': 0.3890227160327417, 'n_estimators': 296, 'min_child_weight': 9, 'gamma': 0.8212579618438315, 'subsample': 0.7724579642158468, 'colsample_bytree': 0.8115801684195623, 'reg_alpha': 0.9179034273673592, 'reg_lambda': 0.152662091270164}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:19,930]\u001b[0m Trial 158 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:26,461]\u001b[0m Trial 159 finished with value: 0.7420889718861119 and parameters: {'max_depth': 15, 'learning_rate': 0.32441689469973617, 'n_estimators': 281, 'min_child_weight': 9, 'gamma': 0.7617324871227604, 'subsample': 0.8134416902298991, 'colsample_bytree': 0.8229626598919483, 'reg_alpha': 0.9380397564935498, 'reg_lambda': 0.09237256935591162}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:29,006]\u001b[0m Trial 160 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:35,595]\u001b[0m Trial 161 finished with value: 0.7428052492901179 and parameters: {'max_depth': 15, 'learning_rate': 0.38323875327071, 'n_estimators': 295, 'min_child_weight': 9, 'gamma': 0.8313551860952549, 'subsample': 0.7680391446821206, 'colsample_bytree': 0.8117727745173933, 'reg_alpha': 0.9179478241529243, 'reg_lambda': 0.13639278075069342}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:41,824]\u001b[0m Trial 162 finished with value: 0.7419354838709677 and parameters: {'max_depth': 15, 'learning_rate': 0.41134602582984775, 'n_estimators': 285, 'min_child_weight': 9, 'gamma': 0.8318809431226966, 'subsample': 0.7670860806521063, 'colsample_bytree': 0.8042818891075825, 'reg_alpha': 0.8732732328237407, 'reg_lambda': 0.08000782292353675}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:42,420]\u001b[0m Trial 163 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:43,443]\u001b[0m Trial 164 pruned. Trial was pruned at iteration 4.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:45,849]\u001b[0m Trial 165 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:46,474]\u001b[0m Trial 166 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:47,063]\u001b[0m Trial 167 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:49,486]\u001b[0m Trial 168 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:56,353]\u001b[0m Trial 169 finished with value: 0.7432912946714079 and parameters: {'max_depth': 15, 'learning_rate': 0.29561583884246667, 'n_estimators': 303, 'min_child_weight': 8, 'gamma': 0.7743621737331092, 'subsample': 0.8197701339490795, 'colsample_bytree': 0.8518997261117187, 'reg_alpha': 0.9199562844443786, 'reg_lambda': 0.2653776327947899}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:56,979]\u001b[0m Trial 170 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:28:57,599]\u001b[0m Trial 171 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:04,818]\u001b[0m Trial 172 finished with value: 0.742242459901256 and parameters: {'max_depth': 15, 'learning_rate': 0.2828622558785502, 'n_estimators': 306, 'min_child_weight': 8, 'gamma': 0.7968009441605127, 'subsample': 0.806917538654786, 'colsample_bytree': 0.8564346339801725, 'reg_alpha': 0.9388363028141382, 'reg_lambda': 0.23015338693005166}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:12,116]\u001b[0m Trial 173 finished with value: 0.7422168785653986 and parameters: {'max_depth': 15, 'learning_rate': 0.3050696170751136, 'n_estimators': 324, 'min_child_weight': 9, 'gamma': 0.7386872120167921, 'subsample': 0.8297344557788091, 'colsample_bytree': 0.8371128063728575, 'reg_alpha': 0.8856042325725695, 'reg_lambda': 0.20720099016692367}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:12,724]\u001b[0m Trial 174 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:13,656]\u001b[0m Trial 175 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:14,155]\u001b[0m Trial 176 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:14,929]\u001b[0m Trial 177 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:18,994]\u001b[0m Trial 178 pruned. Trial was pruned at iteration 46.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:19,711]\u001b[0m Trial 179 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:20,309]\u001b[0m Trial 180 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:20,861]\u001b[0m Trial 181 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:27,519]\u001b[0m Trial 182 finished with value: 0.7433424573431224 and parameters: {'max_depth': 15, 'learning_rate': 0.4102580975392796, 'n_estimators': 296, 'min_child_weight': 9, 'gamma': 0.7924250244698594, 'subsample': 0.767427543874856, 'colsample_bytree': 0.8122906800002003, 'reg_alpha': 0.9162980973628114, 'reg_lambda': 0.16607966409201141}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:28,145]\u001b[0m Trial 183 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:32,348]\u001b[0m Trial 184 pruned. Trial was pruned at iteration 146.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:33,049]\u001b[0m Trial 185 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:33,654]\u001b[0m Trial 186 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:35,592]\u001b[0m Trial 187 pruned. Trial was pruned at iteration 19.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:42,373]\u001b[0m Trial 188 finished with value: 0.7423959479164002 and parameters: {'max_depth': 15, 'learning_rate': 0.41777380054984553, 'n_estimators': 308, 'min_child_weight': 10, 'gamma': 0.7918554165962112, 'subsample': 0.7685395122647057, 'colsample_bytree': 0.8000772963992047, 'reg_alpha': 0.8579306447536629, 'reg_lambda': 0.2136398352499499}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:44,301]\u001b[0m Trial 189 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:57,231]\u001b[0m Trial 190 finished with value: 0.7402982783760969 and parameters: {'max_depth': 15, 'learning_rate': 0.27982351721209936, 'n_estimators': 260, 'min_child_weight': 10, 'gamma': 0.375570434503127, 'subsample': 0.7809255975811001, 'colsample_bytree': 0.8246436317804307, 'reg_alpha': 0.877673215881748, 'reg_lambda': 0.19977504039279947}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:29:59,486]\u001b[0m Trial 191 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:00,069]\u001b[0m Trial 192 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:00,603]\u001b[0m Trial 193 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:01,104]\u001b[0m Trial 194 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:01,945]\u001b[0m Trial 195 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:02,848]\u001b[0m Trial 196 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:04,585]\u001b[0m Trial 197 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:07,183]\u001b[0m Trial 198 pruned. Trial was pruned at iteration 28.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:07,725]\u001b[0m Trial 199 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:08,333]\u001b[0m Trial 200 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:09,083]\u001b[0m Trial 201 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:09,792]\u001b[0m Trial 202 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:10,512]\u001b[0m Trial 203 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:16,776]\u001b[0m Trial 204 finished with value: 0.7435471080299815 and parameters: {'max_depth': 15, 'learning_rate': 0.36674718465137135, 'n_estimators': 273, 'min_child_weight': 9, 'gamma': 0.8381501482198078, 'subsample': 0.7742757416131553, 'colsample_bytree': 0.822428466775699, 'reg_alpha': 0.9177090931875722, 'reg_lambda': 0.17178032149394534}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:17,385]\u001b[0m Trial 205 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:17,966]\u001b[0m Trial 206 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:18,674]\u001b[0m Trial 207 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:21,135]\u001b[0m Trial 208 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:21,733]\u001b[0m Trial 209 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:23,633]\u001b[0m Trial 210 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:24,253]\u001b[0m Trial 211 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:26,221]\u001b[0m Trial 212 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:32,889]\u001b[0m Trial 213 finished with value: 0.7421657158936839 and parameters: {'max_depth': 15, 'learning_rate': 0.3927804282771301, 'n_estimators': 270, 'min_child_weight': 6, 'gamma': 0.8287784437679362, 'subsample': 0.7624495197003872, 'colsample_bytree': 0.805197084876325, 'reg_alpha': 0.7780153071931762, 'reg_lambda': 0.16065744415394417}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:33,655]\u001b[0m Trial 214 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:34,340]\u001b[0m Trial 215 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:35,214]\u001b[0m Trial 216 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:35,886]\u001b[0m Trial 217 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:36,592]\u001b[0m Trial 218 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:37,141]\u001b[0m Trial 219 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:40,036]\u001b[0m Trial 220 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:40,664]\u001b[0m Trial 221 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:41,291]\u001b[0m Trial 222 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:41,918]\u001b[0m Trial 223 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:42,623]\u001b[0m Trial 224 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:43,270]\u001b[0m Trial 225 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:43,866]\u001b[0m Trial 226 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:45,936]\u001b[0m Trial 227 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:46,513]\u001b[0m Trial 228 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:47,099]\u001b[0m Trial 229 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:49,501]\u001b[0m Trial 230 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:30:56,667]\u001b[0m Trial 231 finished with value: 0.7415006011613926 and parameters: {'max_depth': 15, 'learning_rate': 0.30531729147412917, 'n_estimators': 326, 'min_child_weight': 9, 'gamma': 0.7459839938984721, 'subsample': 0.8358075577973361, 'colsample_bytree': 0.8393815541255714, 'reg_alpha': 0.8899232191318628, 'reg_lambda': 0.20352215469074392}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:03,981]\u001b[0m Trial 232 finished with value: 0.7433168760072651 and parameters: {'max_depth': 15, 'learning_rate': 0.30930521280511963, 'n_estimators': 312, 'min_child_weight': 9, 'gamma': 0.735918326955086, 'subsample': 0.8256758952068184, 'colsample_bytree': 0.8457891789887512, 'reg_alpha': 0.8723556815022184, 'reg_lambda': 0.16400200862080594}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:05,614]\u001b[0m Trial 233 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:06,363]\u001b[0m Trial 234 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:07,109]\u001b[0m Trial 235 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:09,182]\u001b[0m Trial 236 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:09,802]\u001b[0m Trial 237 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:11,889]\u001b[0m Trial 238 pruned. Trial was pruned at iteration 19.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:12,458]\u001b[0m Trial 239 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:13,091]\u001b[0m Trial 240 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:13,727]\u001b[0m Trial 241 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:15,435]\u001b[0m Trial 242 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:15,982]\u001b[0m Trial 243 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:18,011]\u001b[0m Trial 244 pruned. Trial was pruned at iteration 15.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:18,612]\u001b[0m Trial 245 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:21,303]\u001b[0m Trial 246 pruned. Trial was pruned at iteration 32.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:21,922]\u001b[0m Trial 247 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:22,511]\u001b[0m Trial 248 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:24,100]\u001b[0m Trial 249 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:24,731]\u001b[0m Trial 250 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:25,956]\u001b[0m Trial 251 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:31,302]\u001b[0m Trial 252 finished with value: 0.7425494359315443 and parameters: {'max_depth': 15, 'learning_rate': 0.39187313671396984, 'n_estimators': 171, 'min_child_weight': 9, 'gamma': 0.8138916162901138, 'subsample': 0.7400830228259523, 'colsample_bytree': 0.7854598020517285, 'reg_alpha': 0.7695819300652786, 'reg_lambda': 0.05399653848398826}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:36,541]\u001b[0m Trial 253 finished with value: 0.7420122278785398 and parameters: {'max_depth': 15, 'learning_rate': 0.38939875695562765, 'n_estimators': 178, 'min_child_weight': 7, 'gamma': 0.8243385126459205, 'subsample': 0.7548448252304062, 'colsample_bytree': 0.7881375862782505, 'reg_alpha': 0.7517890858993332, 'reg_lambda': 0.05778828279169361}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:37,453]\u001b[0m Trial 254 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:38,328]\u001b[0m Trial 255 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:39,243]\u001b[0m Trial 256 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:40,078]\u001b[0m Trial 257 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:40,753]\u001b[0m Trial 258 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:42,815]\u001b[0m Trial 259 pruned. Trial was pruned at iteration 15.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:45,258]\u001b[0m Trial 260 pruned. Trial was pruned at iteration 28.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:45,767]\u001b[0m Trial 261 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:46,299]\u001b[0m Trial 262 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:46,903]\u001b[0m Trial 263 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:47,556]\u001b[0m Trial 264 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:48,248]\u001b[0m Trial 265 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:50,848]\u001b[0m Trial 266 pruned. Trial was pruned at iteration 19.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:51,458]\u001b[0m Trial 267 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:54,121]\u001b[0m Trial 268 pruned. Trial was pruned at iteration 34.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:54,727]\u001b[0m Trial 269 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:55,347]\u001b[0m Trial 270 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:55,919]\u001b[0m Trial 271 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:56,720]\u001b[0m Trial 272 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:57,277]\u001b[0m Trial 273 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:57,895]\u001b[0m Trial 274 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:58,433]\u001b[0m Trial 275 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:59,149]\u001b[0m Trial 276 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:31:59,883]\u001b[0m Trial 277 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:02,326]\u001b[0m Trial 278 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:02,895]\u001b[0m Trial 279 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:03,469]\u001b[0m Trial 280 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:04,715]\u001b[0m Trial 281 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:05,364]\u001b[0m Trial 282 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:06,069]\u001b[0m Trial 283 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:06,759]\u001b[0m Trial 284 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:09,243]\u001b[0m Trial 285 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:19,142]\u001b[0m Trial 286 pruned. Trial was pruned at iteration 92.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:19,755]\u001b[0m Trial 287 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:21,094]\u001b[0m Trial 288 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:21,670]\u001b[0m Trial 289 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:22,415]\u001b[0m Trial 290 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:23,022]\u001b[0m Trial 291 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:23,684]\u001b[0m Trial 292 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:24,315]\u001b[0m Trial 293 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:26,427]\u001b[0m Trial 294 pruned. Trial was pruned at iteration 18.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:26,982]\u001b[0m Trial 295 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:27,785]\u001b[0m Trial 296 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:29,873]\u001b[0m Trial 297 pruned. Trial was pruned at iteration 16.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:30,464]\u001b[0m Trial 298 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:31,206]\u001b[0m Trial 299 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:31,939]\u001b[0m Trial 300 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:32,547]\u001b[0m Trial 301 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:33,063]\u001b[0m Trial 302 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:34,540]\u001b[0m Trial 303 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:37,159]\u001b[0m Trial 304 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:37,710]\u001b[0m Trial 305 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:38,302]\u001b[0m Trial 306 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:38,913]\u001b[0m Trial 307 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:39,460]\u001b[0m Trial 308 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:40,192]\u001b[0m Trial 309 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:40,850]\u001b[0m Trial 310 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:41,499]\u001b[0m Trial 311 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:42,151]\u001b[0m Trial 312 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:42,727]\u001b[0m Trial 313 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:43,522]\u001b[0m Trial 314 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:45,184]\u001b[0m Trial 315 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:45,751]\u001b[0m Trial 316 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:46,278]\u001b[0m Trial 317 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:46,992]\u001b[0m Trial 318 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:49,235]\u001b[0m Trial 319 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:49,801]\u001b[0m Trial 320 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:50,398]\u001b[0m Trial 321 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:50,993]\u001b[0m Trial 322 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:51,587]\u001b[0m Trial 323 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:52,171]\u001b[0m Trial 324 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:52,802]\u001b[0m Trial 325 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:53,496]\u001b[0m Trial 326 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:54,142]\u001b[0m Trial 327 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:54,770]\u001b[0m Trial 328 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:55,532]\u001b[0m Trial 329 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:57,767]\u001b[0m Trial 330 pruned. Trial was pruned at iteration 18.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:32:59,900]\u001b[0m Trial 331 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:00,476]\u001b[0m Trial 332 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:01,088]\u001b[0m Trial 333 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:03,397]\u001b[0m Trial 334 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:03,964]\u001b[0m Trial 335 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:04,839]\u001b[0m Trial 336 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:06,737]\u001b[0m Trial 337 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:07,309]\u001b[0m Trial 338 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:07,922]\u001b[0m Trial 339 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:08,611]\u001b[0m Trial 340 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:09,112]\u001b[0m Trial 341 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:12,432]\u001b[0m Trial 342 pruned. Trial was pruned at iteration 42.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:12,979]\u001b[0m Trial 343 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:13,572]\u001b[0m Trial 344 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:14,174]\u001b[0m Trial 345 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:14,770]\u001b[0m Trial 346 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:15,347]\u001b[0m Trial 347 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:18,624]\u001b[0m Trial 348 pruned. Trial was pruned at iteration 46.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:19,780]\u001b[0m Trial 349 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:20,336]\u001b[0m Trial 350 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:20,940]\u001b[0m Trial 351 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:21,591]\u001b[0m Trial 352 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:22,323]\u001b[0m Trial 353 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:24,403]\u001b[0m Trial 354 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:26,636]\u001b[0m Trial 355 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:27,157]\u001b[0m Trial 356 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:27,751]\u001b[0m Trial 357 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:28,362]\u001b[0m Trial 358 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:30,659]\u001b[0m Trial 359 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:31,233]\u001b[0m Trial 360 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:31,804]\u001b[0m Trial 361 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:32,316]\u001b[0m Trial 362 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:32,860]\u001b[0m Trial 363 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:33,509]\u001b[0m Trial 364 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:34,157]\u001b[0m Trial 365 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:34,751]\u001b[0m Trial 366 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:38,939]\u001b[0m Trial 367 pruned. Trial was pruned at iteration 46.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:39,423]\u001b[0m Trial 368 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:40,023]\u001b[0m Trial 369 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:40,736]\u001b[0m Trial 370 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:41,340]\u001b[0m Trial 371 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:41,938]\u001b[0m Trial 372 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:44,052]\u001b[0m Trial 373 pruned. Trial was pruned at iteration 16.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:44,608]\u001b[0m Trial 374 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:45,198]\u001b[0m Trial 375 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:45,834]\u001b[0m Trial 376 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:46,452]\u001b[0m Trial 377 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:48,799]\u001b[0m Trial 378 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:49,367]\u001b[0m Trial 379 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:49,951]\u001b[0m Trial 380 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:50,583]\u001b[0m Trial 381 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:51,228]\u001b[0m Trial 382 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:51,817]\u001b[0m Trial 383 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:52,445]\u001b[0m Trial 384 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:53,393]\u001b[0m Trial 385 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:53,931]\u001b[0m Trial 386 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:54,471]\u001b[0m Trial 387 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:55,128]\u001b[0m Trial 388 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:55,778]\u001b[0m Trial 389 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:58,689]\u001b[0m Trial 390 pruned. Trial was pruned at iteration 15.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:59,253]\u001b[0m Trial 391 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:33:59,850]\u001b[0m Trial 392 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:00,441]\u001b[0m Trial 393 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:01,049]\u001b[0m Trial 394 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:01,656]\u001b[0m Trial 395 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:02,235]\u001b[0m Trial 396 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:02,857]\u001b[0m Trial 397 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:08,793]\u001b[0m Trial 398 finished with value: 0.7429843186411196 and parameters: {'max_depth': 15, 'learning_rate': 0.2764983392762768, 'n_estimators': 185, 'min_child_weight': 9, 'gamma': 0.8129044773432044, 'subsample': 0.7790121016567284, 'colsample_bytree': 0.830850789843156, 'reg_alpha': 0.46162810513527386, 'reg_lambda': 0.015904073373398105}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:09,409]\u001b[0m Trial 399 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:12,145]\u001b[0m Trial 400 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:12,664]\u001b[0m Trial 401 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:15,354]\u001b[0m Trial 402 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:16,293]\u001b[0m Trial 403 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:17,023]\u001b[0m Trial 404 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:17,614]\u001b[0m Trial 405 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:20,139]\u001b[0m Trial 406 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:20,696]\u001b[0m Trial 407 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:21,298]\u001b[0m Trial 408 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:21,822]\u001b[0m Trial 409 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:22,505]\u001b[0m Trial 410 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:23,152]\u001b[0m Trial 411 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:23,743]\u001b[0m Trial 412 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:25,339]\u001b[0m Trial 413 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:25,889]\u001b[0m Trial 414 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:26,495]\u001b[0m Trial 415 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:26,988]\u001b[0m Trial 416 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:27,690]\u001b[0m Trial 417 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:28,468]\u001b[0m Trial 418 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:29,040]\u001b[0m Trial 419 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:29,662]\u001b[0m Trial 420 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:30,241]\u001b[0m Trial 421 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:30,829]\u001b[0m Trial 422 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:31,448]\u001b[0m Trial 423 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:31,969]\u001b[0m Trial 424 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:32,593]\u001b[0m Trial 425 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:33,195]\u001b[0m Trial 426 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:35,398]\u001b[0m Trial 427 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:35,963]\u001b[0m Trial 428 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:36,495]\u001b[0m Trial 429 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:37,188]\u001b[0m Trial 430 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:37,766]\u001b[0m Trial 431 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:38,498]\u001b[0m Trial 432 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:39,060]\u001b[0m Trial 433 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:39,700]\u001b[0m Trial 434 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:40,309]\u001b[0m Trial 435 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:42,637]\u001b[0m Trial 436 pruned. Trial was pruned at iteration 23.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:48,310]\u001b[0m Trial 437 pruned. Trial was pruned at iteration 83.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:48,897]\u001b[0m Trial 438 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:49,747]\u001b[0m Trial 439 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:50,321]\u001b[0m Trial 440 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:51,417]\u001b[0m Trial 441 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:52,134]\u001b[0m Trial 442 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:54,504]\u001b[0m Trial 443 pruned. Trial was pruned at iteration 28.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:55,015]\u001b[0m Trial 444 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:55,633]\u001b[0m Trial 445 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:56,230]\u001b[0m Trial 446 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:34:58,301]\u001b[0m Trial 447 pruned. Trial was pruned at iteration 21.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:00,725]\u001b[0m Trial 448 pruned. Trial was pruned at iteration 24.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:01,310]\u001b[0m Trial 449 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:01,898]\u001b[0m Trial 450 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:02,483]\u001b[0m Trial 451 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:05,087]\u001b[0m Trial 452 pruned. Trial was pruned at iteration 22.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:05,678]\u001b[0m Trial 453 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:06,196]\u001b[0m Trial 454 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:06,724]\u001b[0m Trial 455 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:11,013]\u001b[0m Trial 456 pruned. Trial was pruned at iteration 114.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:11,553]\u001b[0m Trial 457 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:12,155]\u001b[0m Trial 458 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:12,742]\u001b[0m Trial 459 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:13,239]\u001b[0m Trial 460 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:13,847]\u001b[0m Trial 461 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:14,488]\u001b[0m Trial 462 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:15,135]\u001b[0m Trial 463 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:15,747]\u001b[0m Trial 464 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:16,378]\u001b[0m Trial 465 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:16,993]\u001b[0m Trial 466 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:17,608]\u001b[0m Trial 467 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:21,665]\u001b[0m Trial 468 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:22,184]\u001b[0m Trial 469 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:32,183]\u001b[0m Trial 470 pruned. Trial was pruned at iteration 114.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:32,732]\u001b[0m Trial 471 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:35,125]\u001b[0m Trial 472 pruned. Trial was pruned at iteration 26.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:45,127]\u001b[0m Trial 473 pruned. Trial was pruned at iteration 92.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:51,142]\u001b[0m Trial 474 finished with value: 0.7430866439845488 and parameters: {'max_depth': 15, 'learning_rate': 0.3747705753062598, 'n_estimators': 284, 'min_child_weight': 9, 'gamma': 0.7818330870602097, 'subsample': 0.8722410401256473, 'colsample_bytree': 0.8148277679153724, 'reg_alpha': 0.9405778435001443, 'reg_lambda': 0.10452053283719506}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:35:59,835]\u001b[0m Trial 475 finished with value: 0.7415006011613926 and parameters: {'max_depth': 15, 'learning_rate': 0.3607131896386043, 'n_estimators': 277, 'min_child_weight': 8, 'gamma': 0.37222071104471255, 'subsample': 0.8835112119554999, 'colsample_bytree': 0.8264852819273603, 'reg_alpha': 0.9588216978524464, 'reg_lambda': 0.09304527054494693}. Best is trial 151 with value: 0.7450819881814228.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:00,382]\u001b[0m Trial 476 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:00,981]\u001b[0m Trial 477 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:01,893]\u001b[0m Trial 478 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:02,490]\u001b[0m Trial 479 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:03,290]\u001b[0m Trial 480 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:03,863]\u001b[0m Trial 481 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:04,368]\u001b[0m Trial 482 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:04,985]\u001b[0m Trial 483 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:05,519]\u001b[0m Trial 484 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:06,140]\u001b[0m Trial 485 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:06,791]\u001b[0m Trial 486 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:08,985]\u001b[0m Trial 487 pruned. Trial was pruned at iteration 18.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:09,520]\u001b[0m Trial 488 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:10,114]\u001b[0m Trial 489 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:10,641]\u001b[0m Trial 490 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:13,055]\u001b[0m Trial 491 pruned. Trial was pruned at iteration 19.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:13,641]\u001b[0m Trial 492 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:14,264]\u001b[0m Trial 493 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:14,912]\u001b[0m Trial 494 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:15,550]\u001b[0m Trial 495 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:16,135]\u001b[0m Trial 496 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:16,756]\u001b[0m Trial 497 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:17,397]\u001b[0m Trial 498 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-04-06 19:36:17,952]\u001b[0m Trial 499 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_y2 = df_y2[: int(len(df_y2) * 0.85)].drop([\"building_id\"], axis=1)\n",
    "dev_y2 = df_y2[int(len(df_y2) * 0.85 ):]\n",
    "\n",
    "def optimize(trial):\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-auc\")\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.25, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-5, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1.0),\n",
    "        'eval_metric': 'auc',\n",
    "        'objective': 'multi:softprob',\n",
    "        'use_label_encoder': False,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0,\n",
    "        'callbacks': [pruning_callback]\n",
    "    }\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(**params)\n",
    "    xgb_clf.fit(train_X_tuned, train_y2,\n",
    "                eval_set=[(dev_X_tuned, dev_y2.drop([\"building_id\"], axis=1))],\n",
    "                verbose=False)\n",
    "\n",
    "    result = xgb_clf.predict(dev_X_tuned)\n",
    "    f1_score = val_acc(dev_y2, result)\n",
    "    # f1_szore = xgb_clf.score(train_X_tuned, train_y2)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            sampler=TPESampler(n_startup_trials=20))\n",
    "study.optimize(optimize, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 500\n",
      "Best trial:\n",
      "  Value: 0.7450819881814228\n",
      "  Params: \n",
      "    max_depth: 15\n",
      "    learning_rate: 0.31079718234242176\n",
      "    n_estimators: 302\n",
      "    min_child_weight: 9\n",
      "    gamma: 0.7690739730134937\n",
      "    subsample: 0.8146642478947701\n",
      "    colsample_bytree: 0.8004949225291711\n",
      "    reg_alpha: 0.8961352915576605\n",
      "    reg_lambda: 0.11635687958658758\n"
     ]
    }
   ],
   "source": [
    "print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value: {}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8004949225291711, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.7690739730134937, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.31079718234242176, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=15, max_leaves=None,\n",
       "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=302, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8004949225291711, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.7690739730134937, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.31079718234242176, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=15, max_leaves=None,\n",
       "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=302, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8004949225291711, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.7690739730134937, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.31079718234242176, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=15, max_leaves=None,\n",
       "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=302, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = trial.params\n",
    "model = xgb.XGBClassifier(**params, tree_method='gpu_hist', gpu_id=0)\n",
    "model.fit(train_X_full_tuned, train_y_full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065539382466552"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(dev_X_tuned)\n",
    "f1_score = val_acc(dev_y2, y_pred)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_test_dropped).tolist()\n",
    "\n",
    "write_out(df_X_test, result, xgboost=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
